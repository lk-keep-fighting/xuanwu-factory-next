# AI 诊断助手 - 常见问题 (FAQ)

## 目录

- [基础问题](#基础问题)
- [使用问题](#使用问题)
- [配置问题](#配置问题)
- [性能问题](#性能问题)
- [故障排查](#故障排查)
- [安全和隐私](#安全和隐私)
- [高级话题](#高级话题)

---

## 基础问题

### Q1: AI 诊断助手是什么？

**A:** AI 诊断助手是一个智能化的服务问题诊断工具，集成在服务详情页中。它使用本地 AI 模型理解您的自然语言描述，自动收集服务状态、日志、指标等信息，并提供专业的诊断建议。

**核心优势：**
- 无需记忆复杂的 kubectl 命令
- 自动关联多个数据源进行分析
- 提供结构化的诊断结果和解决方案
- 本地部署，数据安全可控

---

### Q2: AI 诊断助手可以解决哪些问题？

**A:** AI 诊断助手适用于以下场景：

✅ **启动问题：**
- Pod 启动失败（CrashLoopBackOff）
- 镜像拉取失败（ImagePullBackOff）
- 容器配置错误

✅ **运行时问题：**
- 应用程序报错
- 服务响应慢
- 频繁重启

✅ **资源问题：**
- CPU 使用率高
- 内存不足或泄漏
- 资源限制不合理

✅ **网络问题：**
- 服务连接失败
- 超时错误
- DNS 解析问题

✅ **配置问题：**
- 环境变量缺失
- ConfigMap/Secret 错误
- 挂载路径问题

---

### Q3: AI 诊断助手和传统的日志查看有什么区别？

**A:** 

| 特性 | 传统方式 | AI 诊断助手 |
|------|---------|------------|
| **交互方式** | 命令行或界面操作 | 自然语言对话 |
| **数据收集** | 手动查询多个来源 | 自动收集相关信息 |
| **分析能力** | 需要人工分析 | AI 自动分析和关联 |
| **学习曲线** | 需要学习 kubectl 等工具 | 无需专业知识 |
| **诊断速度** | 取决于经验 | 快速且一致 |
| **建议质量** | 依赖个人经验 | 基于最佳实践 |

**示例对比：**

**传统方式：**
```bash
# 1. 查看 Pod 状态
kubectl get pods -n namespace

# 2. 查看 Pod 事件
kubectl describe pod pod-name -n namespace

# 3. 查看日志
kubectl logs pod-name -n namespace --tail=100

# 4. 查看资源使用
kubectl top pod pod-name -n namespace

# 5. 人工分析所有信息...
```

**AI 诊断助手：**
```
用户："这个服务为什么启动失败？"
AI：自动执行上述所有步骤，并提供分析结果和解决方案
```

---

### Q4: AI 诊断助手是否会替代运维人员？

**A:** 不会。AI 诊断助手是一个**辅助工具**，旨在：

- ✅ 提高诊断效率
- ✅ 降低入门门槛
- ✅ 减少重复性工作
- ✅ 提供标准化的诊断流程

但它**不能替代**：
- ❌ 复杂问题的深度分析
- ❌ 架构设计决策
- ❌ 业务逻辑理解
- ❌ 人工判断和经验

**最佳实践：** 将 AI 诊断助手作为"第一响应者"，快速定位常见问题，复杂问题仍需人工介入。

---

## 使用问题

### Q5: 如何开始使用 AI 诊断助手？

**A:** 三个简单步骤：

1. **打开诊断面板**
   - 访问任意服务详情页
   - 点击"AI 诊断"按钮

2. **描述问题**
   - 用自然语言描述您遇到的问题
   - 例如："服务启动失败了"

3. **查看结果**
   - AI 会自动收集信息并分析
   - 查看诊断结果和建议

详细步骤请参考：[用户使用指南](./AI_DIAGNOSTIC_USER_GUIDE.md#快速开始)

---

### Q6: 如何提出好的问题？

**A:** 遵循以下原则：

**✅ 好的问题：**
- "Pod 一直在 CrashLoopBackOff 状态，怎么回事？"
- "应用日志里报 Connection refused 错误"
- "刚更新配置后服务就启动不了了"
- "CPU 使用率突然升高到 90%"

**❌ 不好的问题：**
- "有问题"（太模糊）
- "不工作"（缺少细节）
- "看看"（目标不明确）

**提问技巧：**
1. **描述症状**：说明观察到的现象
2. **提供上下文**：说明何时发生、做了什么操作
3. **明确目标**：说明想要了解什么

---

### Q7: AI 可以理解哪些语言？

**A:** 当前版本主要支持**中文**，也可以理解简单的英文。

**推荐使用中文：**
- ✅ "这个服务为什么启动失败？"
- ✅ "帮我分析一下日志"

**也支持英文：**
- ✅ "Why is the service failing?"
- ✅ "Check the logs"

**混合使用：**
- ✅ "Pod 为什么 CrashLoopBackOff？"
- ✅ "帮我看看 memory usage"

---

### Q8: AI 响应需要多长时间？

**A:** 响应时间取决于问题复杂度：

| 场景 | 预期时间 | 说明 |
|------|---------|------|
| 简单问候 | < 2 秒 | 无需调用工具 |
| 简单诊断 | 5-10 秒 | 1-2 个工具调用 |
| 复杂诊断 | 10-20 秒 | 3-4 个工具调用 |
| 首次使用 | +10-30 秒 | 模型加载时间 |

**影响因素：**
- AI 模型大小（7B 模型比 3B 慢）
- 服务器性能（CPU vs GPU）
- 日志数据量
- 网络延迟

---

### Q9: 可以同时诊断多个服务吗？

**A:** 当前版本每次只能诊断一个服务。

**当前行为：**
- 每个诊断面板绑定到一个服务
- 可以打开多个浏览器标签页分别诊断不同服务
- 每个会话独立，不会相互影响

**未来计划：**
- 支持跨服务关联分析
- 支持批量诊断
- 支持服务依赖关系分析

---

### Q10: 对话历史会保存吗？

**A:** 当前版本**不会**持久化保存对话历史。

**当前行为：**
- 对话内容仅在会话期间保存在内存中
- 关闭诊断面板后，对话历史会被清除
- 刷新页面会丢失对话历史

**建议：**
- 重要的诊断结果请手动复制保存
- 可以截图保存诊断过程

**未来计划：**
- v1.1.0 将支持会话历史保存
- 支持导出诊断报告
- 支持分享诊断会话

---

## 配置问题

### Q11: 如何配置 AI 诊断助手？

**A:** AI 诊断助手需要配置本地 LLM 服务。

**推荐方案：使用 Ollama**

1. **安装 Ollama**
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **启动服务**
   ```bash
   ollama serve
   ```

3. **拉取模型**
   ```bash
   ollama pull qwen2.5:7b
   ```

4. **配置环境变量**（可选）
   ```bash
   AI_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=qwen2.5:7b
   ```

详细配置请参考：[快速开始指南](./AI_DIAGNOSTIC_QUICK_START.md)

---

### Q12: 推荐使用哪个 AI 模型？

**A:** 推荐模型取决于您的硬件配置：

| 模型 | 大小 | 性能 | 适用场景 |
|------|------|------|---------|
| **qwen2.5:7b** | 4.7GB | ⭐⭐⭐⭐ | **推荐**，平衡性能和质量 |
| qwen2.5:3b | 2.0GB | ⭐⭐⭐ | 资源受限环境 |
| qwen2.5:14b | 8.9GB | ⭐⭐⭐⭐⭐ | 高性能服务器 |
| llama3.1:8b | 4.7GB | ⭐⭐⭐⭐ | 英文场景更好 |

**选择建议：**
- **开发环境**：qwen2.5:3b（快速响应）
- **生产环境**：qwen2.5:7b（质量和速度平衡）
- **高性能服务器**：qwen2.5:14b（最佳质量）

---

### Q13: 可以使用 OpenAI API 吗？

**A:** 可以，但不推荐用于生产环境。

**配置方法：**
```bash
AI_PROVIDER=openai
OPENAI_API_KEY=sk-your-api-key
OPENAI_MODEL=gpt-4
```

**优点：**
- ✅ 无需本地部署
- ✅ 响应质量高
- ✅ 无需 GPU

**缺点：**
- ❌ 需要网络连接
- ❌ 数据发送到外部
- ❌ 有使用成本
- ❌ 可能有延迟

**建议：**
- 开发测试可以使用 OpenAI
- 生产环境建议使用本地 Ollama

---

### Q14: 如何验证配置是否正确？

**A:** 使用健康检查接口：

```bash
curl http://localhost:3000/api/ai-diagnostic/health
```

**正常响应：**
```json
{
  "status": "healthy",
  "provider": "ollama",
  "model": "qwen2.5:7b",
  "timestamp": "2024-12-05T10:30:00.000Z",
  "responseTime": 123
}
```

**异常响应：**
```json
{
  "status": "unhealthy",
  "provider": "ollama",
  "error": "Connection refused",
  "timestamp": "2024-12-05T10:30:00.000Z"
}
```

也可以运行测试脚本：
```bash
node test-ai-config.js
```

---

### Q15: 显示"模拟模式"是什么意思？

**A:** "模拟模式"表示 AI 模型未正确配置，系统使用模拟响应。

**原因：**
- Ollama 服务未启动
- 模型未下载
- 环境变量配置错误
- WebSocket 服务器未重启

**解决方法：**
1. 确认 Ollama 正在运行
2. 确认模型已下载
3. 检查环境变量配置
4. **重启 WebSocket 服务器**（重要！）
5. 刷新浏览器页面

详细排查步骤：[故障排查指南](./AI_DIAGNOSTIC_TROUBLESHOOTING.md)

---

## 性能问题

### Q16: AI 响应很慢怎么办？

**A:** 可能的原因和解决方法：

**1. 首次使用（模型加载）**
- 原因：首次调用需要加载模型到内存
- 解决：等待 10-30 秒，后续会快很多

**2. CPU 模式运行**
- 原因：没有 GPU，使用 CPU 推理
- 解决：
  - 使用更小的模型（3B 代替 7B）
  - 升级到 GPU 服务器
  - 考虑使用 OpenAI API

**3. 系统资源不足**
- 原因：内存或 CPU 不足
- 解决：
  - 关闭其他应用
  - 增加系统资源
  - 使用更小的模型

**4. 日志数据量大**
- 原因：读取大量日志需要时间
- 解决：系统已限制最多 1000 行，无需额外操作

---

### Q17: 如何提高 AI 响应速度？

**A:** 优化建议：

**硬件优化：**
- ✅ 使用 GPU（速度提升 5-10 倍）
- ✅ 增加内存（至少 8GB）
- ✅ 使用 SSD 存储

**软件优化：**
- ✅ 使用较小的模型（3B 代替 7B）
- ✅ 调整模型参数（降低 max_tokens）
- ✅ 启用模型缓存

**使用优化：**
- ✅ 问题描述简洁明确
- ✅ 避免过于复杂的问题
- ✅ 分步骤提问而不是一次问多个问题

---

### Q18: AI 诊断助手会影响系统性能吗？

**A:** 影响很小，但需要注意：

**资源占用：**
- **CPU**：推理时占用 1-2 核
- **内存**：模型占用 4-8GB（取决于模型大小）
- **网络**：WebSocket 连接，流量很小

**性能影响：**
- ✅ 不影响 Kubernetes 集群
- ✅ 不影响被诊断的服务
- ✅ 仅在使用时占用资源
- ✅ 空闲时资源占用极小

**建议：**
- 在独立的服务器上运行 AI 服务
- 避免在生产 Kubernetes 节点上运行
- 监控资源使用情况

---

## 故障排查

### Q19: 点击"AI 诊断"按钮没有反应？

**A:** 按以下步骤排查：

**1. 检查浏览器控制台**
```
F12 → Console 标签
查看是否有错误信息
```

**2. 检查 WebSocket 连接**
```
F12 → Network 标签 → WS 筛选
查看 WebSocket 连接状态
```

**3. 确认服务器运行**
```bash
# 检查 WebSocket 服务器
ps aux | grep websocket-server

# 如果未运行，启动它
npm run ws:dev
```

**4. 检查端口**
```bash
# 确认端口 3001 可访问
curl http://localhost:3001
```

**5. 清除缓存**
- 硬刷新：Ctrl+F5 (Windows) 或 Cmd+Shift+R (Mac)
- 或使用无痕模式测试

---

### Q20: AI 的回答不准确怎么办？

**A:** 提高准确性的方法：

**1. 改进问题描述**
- ❌ "有问题"
- ✅ "Pod 一直重启，日志显示 OOMKilled"

**2. 提供更多上下文**
- 说明何时开始出现问题
- 说明最近做了什么操作
- 提供相关的错误信息

**3. 分步骤提问**
```
第一步："服务现在是什么状态？"
第二步："能看一下错误日志吗？"
第三步："这个错误是什么原因？"
```

**4. 验证 AI 的分析**
- 不要盲目相信 AI 的建议
- 结合自己的经验判断
- 必要时寻求人工支持

**5. 反馈问题**
- 记录不准确的案例
- 反馈给技术团队
- 帮助改进 AI 模型

---

### Q21: 工具调用失败怎么办？

**A:** 工具调用失败的常见原因：

**1. 权限不足**
```
错误：Forbidden: User cannot get pods
解决：确认您有访问该服务的权限
```

**2. 资源不存在**
```
错误：Pod not found
解决：确认服务和 Pod 存在
```

**3. 网络问题**
```
错误：Connection timeout
解决：检查 Kubernetes API 连接
```

**4. 配置错误**
```
错误：Invalid kubeconfig
解决：检查 Kubernetes 配置
```

**处理建议：**
- AI 会显示具体的错误信息
- 根据错误信息进行排查
- 如果无法解决，联系技术支持

---

## 安全和隐私

### Q22: AI 诊断助手安全吗？

**A:** 是的，AI 诊断助手设计时充分考虑了安全性：

**数据安全：**
- ✅ 本地部署，数据不离开集群
- ✅ 不会发送数据到外部服务（使用 Ollama 时）
- ✅ 对话内容不持久化存储
- ✅ 关闭面板后数据立即清除

**权限控制：**
- ✅ 继承用户的 Kubernetes 权限
- ✅ 只能访问有权限的服务
- ✅ 所有操作都有审计日志

**敏感信息保护：**
- ✅ 自动过滤密码和密钥
- ✅ 敏感配置会被标记
- ✅ 建议不要在对话中输入敏感信息

---

### Q23: 对话内容会被记录吗？

**A:** 取决于配置：

**当前版本（MVP）：**
- ❌ 对话内容**不会**持久化存储
- ✅ 仅在会话期间保存在内存中
- ✅ 关闭面板后立即清除

**审计日志：**
- ✅ 操作记录会被记录（谁、何时、访问了什么）
- ❌ 对话内容**不会**被记录
- ✅ 工具调用会被记录

**未来版本：**
- 可能支持可选的会话历史保存
- 用户可以选择是否保存
- 保存的内容会加密存储

---

### Q24: 可以在生产环境使用吗？

**A:** 可以，但需要注意：

**✅ 适合生产环境的配置：**
- 使用本地 Ollama（不是 OpenAI API）
- 独立的服务器运行 AI 服务
- 配置适当的资源限制
- 启用审计日志
- 定期备份配置

**⚠️ 生产环境注意事项：**
- 不要盲目执行 AI 的建议
- 重要操作前进行验证
- 保持人工监督
- 定期审查审计日志

**❌ 不建议：**
- 在生产 Kubernetes 节点上运行 AI 服务
- 使用外部 API（如 OpenAI）
- 完全依赖 AI 进行故障处理

---

## 高级话题

### Q25: 如何自定义 AI 的行为？

**A:** 当前版本支持有限的自定义：

**可配置项：**
- AI 提供商（Ollama / OpenAI）
- 模型选择
- 基础 URL
- 超时设置

**不可配置项（当前版本）：**
- 系统提示词
- 工具定义
- 响应格式

**未来计划：**
- 支持自定义提示词模板
- 支持添加自定义工具
- 支持调整 AI 参数（temperature 等）

---

### Q26: 可以添加自定义诊断工具吗？

**A:** 当前版本不支持，但已在规划中。

**当前内置工具：**
- getPodStatus：查询 Pod 状态
- getServiceLogs：读取日志
- getResourceMetrics：获取指标
- getDeploymentConfig：查看配置

**未来计划：**
- 支持插件机制
- 允许添加自定义工具
- 支持集成第三方服务

**如果您有需求：**
- 请联系开发团队
- 描述您需要的工具
- 我们会考虑在未来版本中添加

---

### Q27: AI 诊断助手的技术架构是什么？

**A:** 简化的技术架构：

```
前端 (React)
    ↓ WebSocket
WebSocket 服务器 (Node.js)
    ↓ HTTP
AI Agent 服务
    ↓ HTTP
本地 LLM (Ollama)
    ↓ API
Kubernetes 集群
```

**核心组件：**
- **前端**：React + TypeScript + WebSocket
- **后端**：Node.js + Express + WebSocket
- **AI**：Ollama + Qwen 2.5
- **工具**：@kubernetes/client-node

详细设计请参考：[设计文档](../.kiro/specs/ai-diagnostic-assistant/design.md)

---

### Q28: 如何监控 AI 诊断助手的运行状态？

**A:** 监控方法：

**1. 健康检查接口**
```bash
curl http://localhost:3000/api/ai-diagnostic/health
```

**2. 查看日志**
```bash
# WebSocket 服务器日志
tail -f logs/websocket-server.log

# Ollama 日志
tail -f logs/ollama.log
```

**3. 监控指标**
- WebSocket 连接数
- AI 请求响应时间
- 工具调用成功率
- 错误率

**4. 审计日志**
- 查看 AI 诊断操作记录
- 分析使用模式
- 识别异常行为

---

### Q29: 如何贡献或改进 AI 诊断助手？

**A:** 欢迎贡献！

**可以贡献的方面：**
- 🐛 报告 Bug
- 💡 提出功能建议
- 📝 改进文档
- 🧪 提供测试案例
- 🔧 提交代码改进

**贡献流程：**
1. 查看现有 Issue
2. 创建新 Issue 描述问题或建议
3. 等待团队反馈
4. 如果获得批准，提交 Pull Request

**联系方式：**
- 内部协作平台
- 技术支持邮箱
- 开发团队会议

---

### Q30: 未来会有哪些新功能？

**A:** 规划中的功能：

**v1.1.0（近期）：**
- ✨ 自动修复建议（带确认）
- 💾 会话历史保存
- 📤 导出诊断报告
- 🔗 分享诊断会话

**v1.2.0（中期）：**
- 🔔 实时监听和主动告警
- 🔍 跨服务关联分析
- 📚 知识库学习
- 🎯 更多诊断工具

**v2.0.0（长期）：**
- 🤖 自动修复执行
- 🧠 深度学习优化
- 🌐 多集群支持
- 📊 高级分析和报表

**时间线：**
- v1.1.0：预计 1-2 个月
- v1.2.0：预计 3-4 个月
- v2.0.0：预计 6-12 个月

---

## 获取更多帮助

### 相关文档

- 📖 [用户使用指南](./AI_DIAGNOSTIC_USER_GUIDE.md)
- 🚀 [快速开始](./AI_DIAGNOSTIC_QUICK_START.md)
- ⚙️ [完整配置指南](./AI_DIAGNOSTIC_LLM_SETUP.md)
- 🔧 [故障排查](./AI_DIAGNOSTIC_TROUBLESHOOTING.md)
- 📝 [示例场景](./AI_DIAGNOSTIC_EXAMPLES.md)

### 技术支持

如果 FAQ 没有解答您的问题：

1. 查阅完整文档
2. 检查系统日志
3. 联系技术支持团队
4. 提交 Issue

### 反馈渠道

- 📧 技术支持邮箱
- 💬 内部协作平台
- 🎫 工单系统
- 📞 技术支持热线

---

**最后更新**：2024-12-05  
**版本**：1.0.0  
**维护者**：开发团队

**提示**：如果您有新的问题或建议，欢迎反馈给我们，我们会持续更新这个 FAQ 文档。
